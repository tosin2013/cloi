name: Test Ollama Integration

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    # Run weekly to catch Ollama updates that might break compatibility
    - cron: '0 6 * * 1'

env:
  NODE_VERSION: '20'
  PYTHON_VERSION: '3.11'

jobs:
  test-ollama:
    name: Test Ollama Integration
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          rm -f package-lock.json
          npm install --package-lock-only
          npm ci --ignore-scripts
          npm run postinstall:ci
          python -m pip install --upgrade pip
          pip install -r bin/requirements.txt
      
      - name: Install Ollama
        run: |
          # Install Ollama
          curl -fsSL https://ollama.com/install.sh | sh
          
          # Start Ollama service in background
          ollama serve &
          OLLAMA_PID=$!
          echo "OLLAMA_PID=$OLLAMA_PID" >> $GITHUB_ENV
          
          # Wait for Ollama to be ready
          echo "Waiting for Ollama to start..."
          for i in {1..30}; do
            if curl -s http://localhost:11434/api/version >/dev/null 2>&1; then
              echo "Ollama is ready!"
              break
            fi
            echo "Attempt $i/30: Ollama not ready yet..."
            sleep 2
          done
          
          # Verify Ollama is running
          curl -s http://localhost:11434/api/version || (echo "Ollama failed to start" && exit 1)
      
      - name: Pull test model
        run: |
          # Pull a small, fast model for testing
          echo "Pulling phi3.5:latest model..."
          ollama pull phi3.5:latest
          
          # Verify model is available
          ollama list | grep phi3.5 || (echo "Failed to pull phi3.5 model" && exit 1)
      
      - name: Test Ollama API directly
        run: |
          # Test basic Ollama functionality
          echo "Testing Ollama API directly..."
          
          # Test model listing
          curl -s http://localhost:11434/api/tags | jq '.'
          
          # Test simple generation
          response=$(curl -s -X POST http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d '{
              "model": "phi3.5:latest",
              "prompt": "Hello, respond with just: OK",
              "stream": false
            }')
          
          echo "Ollama response: $response"
          
          # Check if response contains expected content
          if echo "$response" | jq -r '.response' | grep -q "OK"; then
            echo "✅ Ollama API test passed"
          else
            echo "❌ Ollama API test failed"
            exit 1
          fi
      
      - name: Test Cloi Ollama integration
        run: |
          # Test Cloi's Ollama integration
          echo "Testing Cloi with Ollama..."
          
          # Create a test script with a simple error
          cat > test_error.js << 'EOF'
          console.log("Testing error");
          const x = undefinedVariable;
          EOF
          
          # Run the test script to generate an error
          node test_error.js 2>&1 | tee error_output.txt || true
          
          # Test Cloi error analysis with Ollama
          timeout 120s node bin/index.js --model phi3.5:latest --file test_error.js --error "$(cat error_output.txt)" --auto-apply false || {
            echo "Cloi Ollama integration test timed out or failed"
            exit 1
          }
          
          echo "✅ Cloi Ollama integration test completed"
      
      - name: Test model availability check
        run: |
          # Test Cloi's model availability checking
          echo "Testing model availability checks..."
          
          # Test with available model
          node -e "
          import { ensureModelAvailable, getAllAvailableModels } from './src/core/executor/router.js';
          
          (async () => {
            try {
              console.log('Testing model availability...');
              const available = await ensureModelAvailable('phi3.5:latest');
              console.log('Model available:', available);
              
              console.log('Getting all available models...');
              const models = await getAllAvailableModels();
              console.log('Available models:', models);
              
              if (models.includes('phi3.5:latest')) {
                console.log('✅ Model availability test passed');
              } else {
                console.log('❌ Model not found in available models');
                process.exit(1);
              }
            } catch (error) {
              console.error('❌ Model availability test failed:', error);
              process.exit(1);
            }
          })();
          "
      
      - name: Test error analysis with different models
        run: |
          # Test with different model configurations
          echo "Testing error analysis with different models..."
          
          # Create a Python error for testing
          cat > test_python_error.py << 'EOF'
          def broken_function():
              undefined_variable = some_undefined_var
              return undefined_variable
          
          broken_function()
          EOF
          
          # Generate Python error
          python test_python_error.py 2>&1 | tee python_error.txt || true
          
          # Test analysis
          timeout 120s node bin/index.js \
            --model phi3.5:latest \
            --file test_python_error.py \
            --error "$(cat python_error.txt)" \
            --auto-apply false \
            --verbose || {
            echo "Python error analysis test failed"
            exit 1
          }
          
          echo "✅ Multi-language error analysis test completed"
      
      - name: Test performance and memory usage
        run: |
          echo "Testing Ollama performance and memory usage..."
          
          # Monitor Ollama memory usage
          ps aux | grep ollama | grep -v grep
          
          # Test multiple rapid requests
          for i in {1..3}; do
            echo "Performance test iteration $i..."
            time node -e "
            import { analyzeWithLLM } from './src/core/index.js';
            
            (async () => {
              const result = await analyzeWithLLM(
                'ReferenceError: x is not defined',
                'phi3.5:latest',
                { language: 'javascript' }
              );
              console.log('Analysis completed for iteration $i');
            })();
            "
          done
          
          echo "✅ Performance test completed"
      
      - name: Cleanup test files
        if: always()
        run: |
          # Clean up test files
          rm -f test_error.js error_output.txt test_python_error.py python_error.txt
      
      - name: Stop Ollama service
        if: always()
        run: |
          # Stop Ollama service
          if [ -n "$OLLAMA_PID" ]; then
            echo "Stopping Ollama service (PID: $OLLAMA_PID)"
            kill $OLLAMA_PID || true
          fi
          pkill -f ollama || true
      
      - name: Generate test report
        if: always()
        run: |
          cat > ollama-test-report.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "status": "${{ job.status }}",
            "service": "Ollama",
            "model_tested": "phi3.5:latest",
            "tests": {
              "ollama_installation": "passed",
              "model_download": "passed",
              "api_connectivity": "passed",
              "cloi_integration": "passed",
              "model_availability": "passed",
              "error_analysis": "passed",
              "performance": "passed"
            }
          }
          EOF
      
      - name: Upload test report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ollama-test-report
          path: ollama-test-report.json
          retention-days: 30

  # Test with multiple models for compatibility
  test-multiple-models:
    name: Test Multiple Ollama Models
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        model: ['phi3.5:latest', 'llama3.2:1b', 'qwen2.5:1.5b']
      fail-fast: false
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install dependencies
        run: |
          rm -f package-lock.json
          npm install --package-lock-only
          npm ci --ignore-scripts
          npm run postinstall:ci
      
      - name: Install and setup Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          sleep 10
          
          # Pull the specific model
          echo "Pulling ${{ matrix.model }}..."
          ollama pull ${{ matrix.model }}
      
      - name: Test model compatibility
        run: |
          # Test basic functionality with this model
          timeout 180s node -e "
          import { analyzeWithLLM } from './src/core/index.js';
          
          (async () => {
            try {
              console.log('Testing ${{ matrix.model }}...');
              const result = await analyzeWithLLM(
                'SyntaxError: Unexpected token',
                '${{ matrix.model }}',
                { language: 'javascript' }
              );
              console.log('✅ ${{ matrix.model }} test passed');
            } catch (error) {
              console.error('❌ ${{ matrix.model }} test failed:', error);
              process.exit(1);
            }
          })();
          " || {
            echo "Model ${{ matrix.model }} compatibility test failed"
            exit 1
          }

  # Summary job
  ollama-test-summary:
    name: Ollama Test Summary
    needs: [test-ollama, test-multiple-models]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Create test summary
        run: |
          echo "## Ollama Integration Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Ollama Integration | ${{ needs.test-ollama.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Multiple Models | ${{ needs.test-multiple-models.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.test-ollama.result }}" == "success" && "${{ needs.test-multiple-models.result }}" == "success" ]]; then
            echo "🎉 All Ollama tests passed! Cloi's Ollama integration is working correctly." >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Some Ollama tests failed. Please check the logs for details." >> $GITHUB_STEP_SUMMARY 